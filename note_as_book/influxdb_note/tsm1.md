# TSM1引擎

## 文件结构
```
├── monitor
│   └── monitor
│       └── 1
│           └── index
│               ├── L0-00000001.tsl
│               └── MANIFEST
└── qj
    ├── 4year
    │   ├── 2
    │   │   ├── 000000004-000000003.tsm
    │   │   ├── 000000009-000000003.tsm
    │   │   ├── 000000011-000000002.tsm
    │   │   └── index
    │   │       ├── L0-00000002.tsl
    │   │       ├── L1-00000001.tsi
    │   │       └── MANIFEST
    │   └── 3
    │       ├── 000000001-000000001.tsm
    │       └── index
    │           ├── L0-00000002.tsl
    │           ├── L1-00000001.tsi
    │           └── MANIFEST
    └── autogen
        ├── 4
        │   └── index
        │       ├── L0-00000001.tsl
        │       └── MANIFEST
        ├── 5
        │   └── index
        │       ├── L0-00000001.tsl
        │       └── MANIFEST
        └── 6
            └── index
                ├── L0-00000001.tsl
                └── MANIFEST
```
在一个shard中的tsm文件按`{generation}-{level}.tsm`格式命名.比如`000000004-000000003.tsm`即是第4代level3文件.

>Level 0 is always created from the result of a cache compaction.  It generates
1 file with a sequence num of 1. 
Level 2 is generated by compacting multiple level 1 files.  
Level 3 is generate by compacting multiple level 2 files.  
Level 4 is for anything else.


## Engine
Engine决定了对一个shard中的数据如何存取.是存储层的基本数据单元（索引不在engine层提供)

```Golang
// Engine represents a storage engine with compressed blocks.
// 一个引擎实例负责一个实际的shard
type Engine struct {
	mu sync.RWMutex

	// The following group of fields is used to track the state of level compactions within the
	// Engine. The WaitGroup is used to monitor the compaction goroutines, the 'done' channel is
	// used to signal those goroutines to shutdown. Every request to disable level compactions will
	// call 'Wait' on 'wg', with the first goroutine to arrive (levelWorkers == 0 while holding the
	// lock) will close the done channel and re-assign 'nil' to the variable. Re-enabling will
	// decrease 'levelWorkers', and when it decreases to zero, level compactions will be started
	// back up again.

	wg           sync.WaitGroup // waitgroup for active level compaction goroutines
	done         chan struct{}  // channel to signal level compactions to stop
	levelWorkers int            // Number of "workers" that expect compactions to be in a disabled state

	snapDone chan struct{}  // channel to signal snapshot compactions to stop
	snapWG   sync.WaitGroup // waitgroup for running snapshot compactions

	id           uint64     // shard id
	database     string
	path         string
	logger       zap.Logger // Logger to be used for important messages
	traceLogger  zap.Logger // Logger to be used when trace-logging is on.
	traceLogging bool

	// index不在引擎层所以需要从上层传入本shard的index和fieldset
	index    tsdb.Index
	fieldset *tsdb.MeasurementFieldSet

	WAL            *WAL
	Cache          *Cache
	Compactor      *Compactor
	CompactionPlan CompactionPlanner
	FileStore      *FileStore			// 每个shard都有多个文件组成， FileStore负责管理这些文件

	MaxPointsPerBlock int 				// 构造Block的阈值

	// CacheFlushMemorySizeThreshold specifies the minimum size threshodl for
	// the cache when the engine should write a snapshot to a TSM file
	CacheFlushMemorySizeThreshold uint64

	// CacheFlushWriteColdDuration specifies the length of time after which if
	// no writes have been committed to the WAL, the engine will write
	// a snapshot of the cache to a TSM file
	CacheFlushWriteColdDuration time.Duration

	// Controls whether to enabled compactions when the engine is open
	enableCompactionsOnOpen bool

	stats *EngineStatistics

	// Limiter for concurrent compactions.
	// 使用带缓存的chan来控制compaction并发线程
	//
	// <-limiter.C
	// do compaction
	// limiter.C <-
	compactionLimiter limiter.Fixed

	scheduler *scheduler
}
```

## 写入数据
`WritePoints` 将写入的Point切片分拣到｀values(map[key][]Value)｀中，然后调用`Cache.WriteMulti`和`WAL.WriteMulti`分别写到Cache、WAL中.

```Golang
func (e *Engine) WritePoints(points []models.Point) error {
	// map[key][]Value
	values := make(map[string][]Value, len(points))
	var keyBuf []byte
	var baseLen int
	for _, p := range points {
		keyBuf = append(keyBuf[:0], p.Key()...)
		keyBuf = append(keyBuf, keyFieldSeparator...)
		baseLen = len(keyBuf)
		iter := p.FieldIterator()
		t := p.Time().UnixNano()
		for iter.Next() {
			// Skip fields name "time", they are illegal
			if bytes.Equal(iter.FieldKey(), timeBytes) {
				continue
			}

			keyBuf = append(keyBuf[:baseLen], iter.FieldKey()...)
			var v Value
			switch iter.Type() {
			case models.Float:
				fv, err := iter.FloatValue()
				if err != nil {
					return err
				}
				v = NewFloatValue(t, fv)
			case models.Integer:
				iv, err := iter.IntegerValue()
				if err != nil {
					return err
				}
				v = NewIntegerValue(t, iv)
			case models.Unsigned:
				iv, err := iter.UnsignedValue()
				if err != nil {
					return err
				}
				v = NewUnsignedValue(t, iv)
			case models.String:
				v = NewStringValue(t, iter.StringValue())
			case models.Boolean:
				bv, err := iter.BooleanValue()
				if err != nil {
					return err
				}
				v = NewBooleanValue(t, bv)
			default:
				return fmt.Errorf("unknown field type for %s: %s", string(iter.FieldKey()), p.String())
			}
			values[string(keyBuf)] = append(values[string(keyBuf)], v)
		}
	}

	e.mu.RLock()
	defer e.mu.RUnlock()

	// first try to write to the cache
	err := e.Cache.WriteMulti(values)
	if err != nil {
		return err
	}

	_, err = e.WAL.WriteMulti(values)
	return err
}

```

### Cache的写入
Cache对象维护统计信息（写入出错数、写入的数据规模(Size),写入数据数)、锁和执行容量检查,有自己的store接口代理实际数据存取.

有个问题：如果写入过快在compaction之前可能触发ErrCacheMemorySizeLimitExceeded
```golang
// Cache maintains an in-memory store of Values for a set of keys.
type Cache struct {
	// Due to a bug in atomic  size needs to be the first word in the struct, as
	// that's the only place where you're guaranteed to be 64-bit aligned on a
	// 32 bit system. See: https://golang.org/pkg/sync/atomic/#pkg-note-BUG
	size         uint64
	snapshotSize uint64

	mu      sync.RWMutex
	store   storer
	maxSize uint64

	// snapshots are the cache objects that are currently being written to tsm files
	// they're kept in memory while flushing so they can be queried along with the cache.
	// they are read only and should never be modified
	snapshot     *Cache
	snapshotting bool

	// This number is the number of pending or failed WriteSnaphot attempts since the last successful one.
	snapshotAttempts int

	stats        *CacheStatistics
	lastSnapshot time.Time

	// A one time synchronization used to initial the cache with a store.  Since the store can allocate a
	// a large amount memory across shards, we lazily create it.
	initialize       atomic.Value
	initializedCount uint32
}

// WriteMulti writes the map of keys and associated values to the cache. This
// function is goroutine-safe. It returns an error if the cache will exceeded
// its max size by adding the new values.  The write attempts to write as many
// values as possible.  If one key fails, the others can still succeed and an
// error will be returned.
func (c *Cache) WriteMulti(values map[string][]Value) error {
	// init initializes the cache and allocates the underlying store.  
	// Once initialized,the store re-used until Freed.
	// 	func (c *Cache) init() {
	//      if !atomic.CompareAndSwapUint32(&c.initializedCount, 0, 1) {
	// 	        return
	//      }
	// 		c.mu.Lock()
	// 		c.store, _ = newring(ringShards)
	// 		c.mu.Unlock()
	//  }
	c.init()
	var addedSize uint64
	for _, v := range values {
		addedSize += uint64(Values(v).Size())
	}

	// Enough room in the cache?
	limit := c.maxSize // maxSize is safe for reading without a lock.
	n := c.Size() + addedSize
	if limit > 0 && n > limit {
		atomic.AddInt64(&c.stats.WriteErr, 1)
		return ErrCacheMemorySizeLimitExceeded(n, limit)
	}

	var werr error
	c.mu.RLock()
	store := c.store
	c.mu.RUnlock()

	// We'll optimistically set size here, and then decrement it for write errors.
	c.increaseSize(addedSize)
	for k, v := range values {
		newKey, err := store.write([]byte(k), v)
		if err != nil {
			// The write failed, hold onto the error and adjust the size delta.
			werr = err
			addedSize -= uint64(Values(v).Size())
			c.decreaseSize(uint64(Values(v).Size()))
		}
		if newKey {
			addedSize += uint64(len(k))
			c.increaseSize(uint64(len(k)))
		}
	}

	// Some points in the batch were dropped.  An error is returned so
	// error stat is incremented as well.
	if werr != nil {
		atomic.AddInt64(&c.stats.WriteDropped, 1)
		atomic.AddInt64(&c.stats.WriteErr, 1)
	}

	// Update the memory size stat
	c.updateMemSize(int64(addedSize))
	atomic.AddInt64(&c.stats.WriteOK, 1)

	return werr
}
```

store 实现为一个hash-ring共4096个partition,每个partition有一个map[key]*entry.
key经过hash取模后被分到某个分区中，每个分区存放hash值相同的一类key和values.
```golang
type ring struct {
	// Number of keys within the ring. This is used to provide a hint for
	// allocating the return values in keys(). It will not be perfectly accurate
	// since it doesn't consider adding duplicate keys, or trying to remove non-
	// existent keys.
	keysHint int64

	// The unique set of partitions in the ring.
	partitions []*partition
}

// partition provides safe access to a map of series keys to entries.
type partition struct {
	mu    sync.RWMutex
	store map[string]*entry
}

// entry is a set of values and some metadata.
type entry struct {
	mu     sync.RWMutex
	values Values // All stored values.

	// The type of values stored. Read only so doesn't need to be protected by
	// mu.
	vtype int
}
```

### Compactions
引擎通过Compaction将CacheSnapshot写为新的tsm文件(SnapshotCompactions),将低level的tsm文件归并为更高level的tsm文件(LevelCompactions).
外部需要在向engine写入数据前调用SetCompactionsEnabled方法确保激活compactions过程.
```golang
// SetCompactionsEnabled enables compactions on the engine.  When disabled
// all running compactions are aborted and new compactions stop running.
func (e *Engine) SetCompactionsEnabled(enabled bool) {
	if enabled {
		e.enableSnapshotCompactions()
		e.enableLevelCompactions(false)
	} else {
		e.disableSnapshotCompactions()
		e.disableLevelCompactions(false)
	}
}
```
#### compact cache
在一个线程中使用ticker周期性调用WriteSnapshot:
- wal.CloseSegment(): 关闭wal当前段.
- 创建CacheSnapshot:snapshot = cache.store; cache.store.reset()
- snapshot.Deduplicate():对store中的entry进行排序,保证快照数据有序.
- writeSnapshotAndCommit(walSegments, snapshot)
	- Compactor.WriteSnapshot(snapshot):使用Compactor去并发地compact快照，生成一批tsm文件.
	- WAL.Remove(closedFiles):快照已compact，清理对应的walSegments
```golang
// compactCache continually checks if the WAL cache should be written to disk.
func (e *Engine) compactCache(quit <-chan struct{}) {
	t := time.NewTicker(time.Second)
	defer t.Stop()
	for {
		select {
		case <-quit:
			return

		case <-t.C:
			e.Cache.UpdateAge()
			if e.ShouldCompactCache(e.WAL.LastWriteTime()) {
				start := time.Now()
				e.traceLogger.Info(fmt.Sprintf("Compacting cache for %s", e.path))
				err := e.WriteSnapshot()
				if err != nil && err != errCompactionsDisabled {
					e.logger.Info(fmt.Sprintf("error writing snapshot: %v", err))
					atomic.AddInt64(&e.stats.CacheCompactionErrors, 1)
				} else {
					atomic.AddInt64(&e.stats.CacheCompactions, 1)
				}
				atomic.AddInt64(&e.stats.CacheCompactionDuration, time.Since(start).Nanoseconds())
			}
		}
	}
}
```

#### compact
compact过程非常复杂晦涩难懂.具体见PlanLevel中的计划计算.
```
t0: 1_1;2_1;3_1;4_1;5_1;6_1;7_1;8_1;9_1;10_1
t1: 9_1;10_1;11_2(三个段都成为孤儿需要等到full compaction的时候才会被合并)
t2: 9_1;10_1;11_2;12_1;13_1;14_1;15_1;16_1;17_1;18_1;19_1;20_1
t3: 9_1;10_1;11_2;(12_1;13_1;14_1;15_1;16_1;17_1;18_1;19_1);20_1
t3: 9_1;10_1;11_2;20_1;21_2
```

主流程:
```golang
func (e *Engine) compact(quit <-chan struct{}) {
	t := time.NewTicker(time.Second)
	defer t.Stop()
	for {
		select {
		case <-quit:
			return
		case <-t.C:
			// 计算每一level的Compact计划
			level1Groups := e.CompactionPlan.PlanLevel(1)
			level2Groups := e.CompactionPlan.PlanLevel(2)
			level3Groups := e.CompactionPlan.PlanLevel(3)
			level4Groups := e.CompactionPlan.Plan(e.WAL.LastWriteTime())
			...
			// 分层compact
			if level, runnable := e.scheduler.next(); runnable {
				...
				switch level {
				case 1:
					if e.compactHiPriorityLevel(level1Groups[0], 1) {
						level1Groups = level1Groups[1:]
					}
				case 2:
					if e.compactHiPriorityLevel(level2Groups[0], 2) {
						level2Groups = level2Groups[1:]
					}
				case 3:
					if e.compactLoPriorityLevel(level3Groups[0], 3) {
						level3Groups = level3Groups[1:]
					}
				case 4:
					if e.compactFull(level4Groups[0]) {
						level4Groups = level4Groups[1:]
					}
				}
			}

			// Release all the plans we didn't start.
			e.CompactionPlan.Release(level1Groups)
			e.CompactionPlan.Release(level2Groups)
			e.CompactionPlan.Release(level3Groups)
			e.CompactionPlan.Release(level4Groups)
		}
	}
}
```

### WAL的写入
WriteWALEntry对象持有需要写入的一批数据，负责序列化。

writeToLog将entry序列化后的数据使用snappy算法压缩写入文件（在写入前检查是否需要创建新的segment(rollSegment)).

并发写入后调用scheduleSync确保开启了sync线程进行fsync在sync完成后通知调用者(`基于chan的发布订阅`).sync线程由一个ticker触发周期性fsync操作，如果没有调用者等待同步结果则会退出（我觉得没有必要，可以一直保持周期性动作)

```golang
// WriteMulti writes the given values to the WAL. It returns the WAL segment ID to
// which the points were written. If an error is returned the segment ID should
// be ignored.
func (l *WAL) WriteMulti(values map[string][]Value) (int, error) {
	entry := &WriteWALEntry{
		Values: values,
	}

	id, err := l.writeToLog(entry)
	if err != nil {
		atomic.AddInt64(&l.stats.WriteErr, 1)
		return -1, err
	}
	atomic.AddInt64(&l.stats.WriteOK, 1)

	return id, nil
}

// scheduleSync will schedule an fsync to the current wal segment and notify any
// waiting gorutines.  If an fsync is already scheduled, subsequent calls will
// not schedule a new fsync and will be handle by the existing scheduled fsync.
func (l *WAL) scheduleSync() {
	// If we're not the first to sync, then another goroutine is fsyncing the wal for us.
	if !atomic.CompareAndSwapUint64(&l.syncCount, 0, 1) {
		return
	}

	// Fsync the wal and notify all pending waiters
	go func() {
		var timerCh <-chan time.Time

		// time.NewTicker requires a > 0 delay, since 0 indicates no delay, use a closed
		// channel which will always be ready to read from.
		if l.syncDelay == 0 {
			// Create a RW chan and close it
			timerChrw := make(chan time.Time)
			close(timerChrw)
			// Convert it to a read-only
			timerCh = timerChrw
		} else {
			t := time.NewTicker(l.syncDelay)
			defer t.Stop()
			timerCh = t.C
		}
		for {
			select {
			case <-timerCh:
				l.mu.Lock()
				if len(l.syncWaiters) == 0 {
					atomic.StoreUint64(&l.syncCount, 0)
					l.mu.Unlock()
					return
				}

				l.sync()
				l.mu.Unlock()
			case <-l.closing:
				atomic.StoreUint64(&l.syncCount, 0)
				return
			}
		}
	}()
}
// sync fsyncs the current wal segments and notifies any waiters.  Callers must ensure
// a write lock on the WAL is obtained before calling sync.
func (l *WAL) sync() {
	err := l.currentSegmentWriter.sync()
	for len(l.syncWaiters) > 0 {
		errC := <-l.syncWaiters
		errC <- err
	}
}
```

### 内存管理


### 技巧
- buf
```golang
keyBuf = append(keyBuf[:0], p.Key()...) // 复用keyBuf,keyBuf不够的时候可以自动扩展
keyBuf = append(keyBuf[:baseLen], iter.FieldKey()...)
```

- 仅包内实现接口
使用非导出的接口函数签名保证了Value仅仅在包内可以实现它
```Golang
// Value represents a TSM-encoded value.
type Value interface {
	// UnixNano returns the timestamp of the value in nanoseconds since unix epoch.
	UnixNano() int64

	// Value returns the underlying value.
	Value() interface{}

	// Size returns the number of bytes necessary to represent the value and its timestamp.
	Size() int

	// String returns the string representation of the value and its timestamp.
	String() string

	// internalOnly is unexported to ensure implementations of Value
	// can only originate in this package.
	internalOnly()
}
```

- 频繁调用的锁优化
对于需要频繁调用且需要锁且读多写少的函数通过顺序加读锁、加写锁来减少对写锁的请求。
例如enableSnapshotCompactions的实现:
```golang
func (e *Engine) enableSnapshotCompactions() {
	// Check if already enabled under read lock
	e.mu.RLock()
	if e.snapDone != nil {
		e.mu.RUnlock()
		return
	}
	e.mu.RUnlock()

	// Check again under write lock
	e.mu.Lock()
	if e.snapDone != nil {
		e.mu.Unlock()
		return
	}

	e.Compactor.EnableSnapshots()
	quit := make(chan struct{})
	e.snapDone = quit
	e.snapWG.Add(1)
	e.mu.Unlock()

	go func() { defer e.snapWG.Done(); e.compactCache(quit) }()
}
```

- Static objects to prevent small allocs.
```golang
// Static objects to prevent small allocs.
var (
	timeBytes              = []byte("time")
	keyFieldSeparatorBytes = []byte(keyFieldSeparator)
)
```

- 时间哨兵
CompactPlaner需要定时检查各代tsm文件指定compact计划（读取频繁）但tsm文件修改却不是经常发生的，所以对第一步的读取结果做了缓存和时间标记，下次再读取的时候只需要检查缓存时间标和filestore的最后修改时间便可确定是否需要重算结果.

- 基于chan实现pool
    - 优点:
		- 带缓冲的chan提供了稳定的池容量
		- 申请到cap不够的buf时创建新的
		- 归还的buf大于阈值时不池化以控制池膨胀
	- 缺点：
		- 池中的buf尺寸顺序取决于Put的顺序，Get到的buf可能难以命中尺寸要求(尤其是buf尺寸范围宽的时候)

```golang
// Get returns a byte slice size with at least sz capacity. Items
// returned may not be in the zero state and should be reset by the
// caller.
func (p *LimitedBytes) Get(sz int) []byte {
	var c []byte

	// If we have not allocated our capacity, return a new allocation,
	// otherwise block until one frees up.
	select {
	case c = <-p.pool:
	default:
		return make([]byte, sz)
	}

	if cap(c) < sz {
		return make([]byte, sz)
	}

	return c[:sz]
}

// Put returns a slice back to the pool.  If the pool is full, the byte
// slice is discarded.  If the byte slice is over the configured max size
// of any byte slice in the pool, it is discared.
func (p *LimitedBytes) Put(c []byte) {
	// Drop buffers that are larger than the max size
	if cap(c) >= p.maxSize {
		return
	}

	select {
	case p.pool <- c:
	default:
	}
}
```